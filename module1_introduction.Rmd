---
title: 'Module 1: Introduction'
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

In this module, we will go through a publicly released dataset from the PheCAP R package to get hands-on experience with phenotyping.  The goal of phenotyping is to infer a patients phenotype based on the information in their electronic health record (EHR).

```{r}
# Packages required for this module.
packages <- c("tidyverse", "PheCAP", "corrplot", "ggplot2")

# Load packages, or install if missing.
package.check <- lapply(
	  packages,
	  FUN = function(x) {
	    if (!require(x, character.only = TRUE)) {
	      install.packages(x, dependencies = TRUE)
	      library(x, character.only = TRUE)
	    }
	  }
	)
```

```{r}
# Load helper functions.
source("../Rscripts/helper_function.R")
```

# PheCAP 

Information about the PheCAP package can be found here: https://celehs.github.io/PheCAP/.

This data mart is a random sample of patients in the Mass General Brigham (formerly Partner's Healthcare) EHR database who had at lease one note of 500 characters:

(i) met an initial filter for coronary artery disease (CAD): $\geq$ 1 ICD9 code related to CAD (410.x, 411.x, 412.x, 413., 414.x) 

or

(ii) $\ge$ 1 mention for any CAD related concepts (eg. CAD, CAD procedures, CAD biomarkers, positive stress test)

# PheCAP data

```{r}
data(ehr_data)
data <- PhecapData(ehr_data, "healthcare_utilization", "label", 0.4, patient_id = "patient_id")
data
```

The data contains 10,000 observations, but only 181 labeled examples split into a training and validation set. 

# Exploratory data analysis

```{r, eval = FALSE}
ehr_data %>% head() 
```

- Labels: "label", whether the patient has CAD, **extracted from chart review by a clinician**

- Features:
  - "main_ICD", "main_NLP" refers to total number of billing codes or NLP mentions of the disease
  - "healthcare_utilization" refers to total number of notes the patient has

- Features: “CODx” (n = 10), “NLPx” (n = 574) refers to the counts of a specific code or NLP term, respectively 

## Missingness

```{r}
colnames(ehr_data)[which( colMeans(is.na(ehr_data)) > 0)]
```

There is no missing data, except what is in the label.

## Prevalence of CAD

```{r}
mean(ehr_data$label, na.rm = TRUE)
```

## Distributions of the features

```{r}
set.seed(99)
feature_index <- sample(c(3:ncol(ehr_data)), 10, replace = FALSE)
ehr_data[, feature_index] %>%
  pivot_longer(everything(), names_to = "feature", values_to = "count") %>%
  ggplot() +
  geom_density(aes(x = count, color = feature))
```

The features are highly skewed.  

## Correlation among the features

```{r}
features <- ehr_data[, c(3:ncol(ehr_data))]
feature_cor <- cor(features[feature_index], method = "spearman")
corrplot::corrplot(feature_cor)
```

# Healthcare utilization

"Healthcare_utilization" refers to total number of notes the patient has. 

```{r}
surrogate_index <- sample(3:ncol(ehr_data), 2)

ehr_data %>%
  dplyr::select(label, healthcare_utilization, surrogate_index) %>%
  filter(!is.na(label)) %>%
  mutate(disease = factor(label)) %>%
  dplyr::select(-label) %>%
  pivot_longer(c(everything(), -disease, -healthcare_utilization), 
               names_to = "feature", values_to = "count") %>%
  ggplot(aes(x = healthcare_utilization, y = log(count + 1), color = disease, shape = feature, linetype = feature)) +
  geom_point() +
  geom_smooth(method='lm', se = FALSE, size=1)
```

Increased healthcare utilization leads to higher features counts in both the cases and controls. 


# Basic supervised learning.

```{r}
# Data with non-missing labels
labeled_data <- ehr_data %>% dplyr::filter(!is.na(label))


# All Features
all_x <- log(ehr_data %>% dplyr::select(
  starts_with("COD"), starts_with("NLP"),
  starts_with("main"), healthcare_utilization) + 1
)

# Training Set
train_data <- ehr_data %>% dplyr::filter(patient_id %in% data$training_set)
train_x <- train_data %>%
  dplyr::select(
    starts_with("COD"), starts_with("NLP"),
    starts_with("main"), healthcare_utilization
  ) %>%
  as.matrix()
train_y <- train_data %>%
  dplyr::select(label) %>%
  pull()

# Testing Set
test_data <- ehr_data %>% dplyr::filter(patient_id %in% data$validation_set)
test_x <- test_data %>%
  dplyr::select(
    starts_with("COD"), starts_with("NLP"),
    starts_with("main"), healthcare_utilization
  ) %>%
  as.matrix()
test_y <- test_data %>%
  dplyr::select(label) %>%
  pull()
```

-   Fit LASSO and Adaptive LASSO(ALASSO)

```{r}
# Choose best lambda using CV
beta.lasso <- lasso_fit(
  x = train_x, y = train_y,
  tuning = "cv", family = "binomial"
)
```


```{r}
# Features Selected
names(beta.lasso[abs(beta.lasso) > 0])[-1]
```


```{r}
# prediction on testing set
y_hat.lasso <- linear_model_predict(
  beta = beta.lasso, x = test_x,
  probability = TRUE
)
```

```{r}
# Fit Adaptive LASSO
beta.alasso <- adaptive_lasso_fit(
  x = train_x, y = train_y,
  tuning = "cv", family = "binomial"
)
y_hat.alasso <- linear_model_predict(
  beta = beta.alasso, x = test_x,
  probability = TRUE
)


# Features Selected
names(beta.alasso[abs(beta.alasso) > 0])[-1]
```
```{r}
roc.lasso <- roc(test_y, y_hat.lasso)
roc.alasso <- roc(test_y, y_hat.alasso)

plot(roc.lasso,
  print.auc = TRUE, main = paste0("n_training = ", dim(train_x)[1])
)
plot(roc.alasso,
  print.auc = TRUE, col = "red", add = TRUE, print.auc.y = 0.4
)
legend(0, 0.2,
  legend = c("LASSO", "ALASSO"), col = c("black", "red"),
  lty = 1, cex = 0.8
)
```


# Accuracy of the surrogates.




