---
title: "Example code"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load packages.
library(PheCAP)
library(tidyverse)
library(randomForestSRC)
library(PheNorm)
library(MAP)
```

```{r}
# Load helper functions.
source("../Rscripts/rankCor.R")
source("../Rscripts/extremeMethod.R")
source("../Rscripts/clusteringMethod.R")
source("../Rscripts/modelFitting.R")
```

```{r}
data(ehr_data)
data <- PhecapData(ehr_data, "healthcare_utilization", "label", 0.4, patient_id = "patient_id")
data
```

# Module 1: Feature selection. 

```{r}
# Prepare data for feature selection, transformed. 
sicd <- log(ehr_data$main_ICD + 1)
snlp <- log(ehr_data$main_NLP + 1)
x <- data.matrix(ehr_data %>% dplyr::select(starts_with("COD") | starts_with("NLP")))
x <- log(x + 1)
```

## Rank correlation. 

Use `main_nlp` as a surrogate to select features. 

```{r}
AFEP_select <- rankCor(snlp, x, threshold = 0.15)
AFEP_feature <- colnames(x)[AFEP_select]
AFEP_feature
```

Let's try other parameters. What do you observe?

```{r}
AFEP_select0.3 <- rankCor(snlp, x, threshold = 0.3)
AFEP_feature0.3 <- colnames(x)[AFEP_select0.3]
AFEP_feature0.3
```

```{r}
AFEP_select0.5 <- rankCor(snlp, x, threshold = 0.5)
AFEP_feature0.5 <- colnames(x)[AFEP_select0.5]
AFEP_feature0.5
```

```{r}
AFEP_select0.1 <- rankCor(snlp, x, threshold = 0.1)
AFEP_feature0.1 <- colnames(x)[AFEP_select0.1]
AFEP_feature0.1
```

## Tail method. 

```{r}
# Tail method.
SAFE_icd <- extreme_method(sicd, x, u_bound = log(11), l_bound = log(1))
SAFE_nlp <- extreme_method(snlp, x, u_bound = log(11), l_bound = log(1))
SAFE_both <- extreme_method(cbind(sicd, snlp), x, u_bound = log(11), l_bound = log(1))
beta <- rbind(SAFE_icd$beta_all, SAFE_nlp$beta_all, SAFE_both$beta_all)
SAFE_select <- which(colMeans(beta, na.rm = T) >= 0.5)
SAFE_feature <- colnames(x)[SAFE_select]
SAFE_feature
```

We select features that occur 50% among the three different surrogate-selected feature sets.
This is the idea of majority voting. 

We can try other prevalence. 

```{r}
# Tail method.
SAFE_select0 <- which(colMeans(beta, na.rm = T) >= 0)
SAFE_feature0 <- colnames(x)[SAFE_select0]
SAFE_feature0
```

```{r}
# Tail method.
SAFE_select0.3 <- which(colMeans(beta, na.rm = T) >= 0.3)
SAFE_feature0.3 <- colnames(x)[SAFE_select0.3]
SAFE_feature0.3
```

```{r}
# Tail method.
SAFE_select0.8 <- which(colMeans(beta, na.rm = T) >= 0.8)
SAFE_feature0.8 <- colnames(x)[SAFE_select0.8]
SAFE_feature0.8
```

## Cluster method. 

```{r}
Auto <- clustering_method(cbind(sicd, snlp), x)
Auto_select <- Auto$beta_select
Auto_feature <- colnames(x)[Auto_select]
Auto_feature
```

Other options?

[Note: to include the parameters for clustering method]

## Is there any features selected by all the methods?

```{r}
feature <- c()
feature <- rbind(feature, 1:ncol(x) %in% AFEP_select)
feature <- rbind(feature, 1:ncol(x) %in% SAFE_select)
feature <- rbind(feature, 1:ncol(x) %in% Auto_select)

rownames(feature) <- c("Rank correlation", 
                       "Tail method", 
                       "Cluster method")

UpSetR::upset(data.frame(t(feature*1)), order.by = "freq") 
```

Try other feature set. e.g. AFEP_select0.3.


---------------------------------------------------------------------------------------------------
# Module 2: Feature selection. 

## Prepare data for algorithm development 

- Split data into training and testing set

```{r}
all_x <- ehr_data %>% dplyr::select(starts_with("COD"), starts_with("NLP"), 
                                 starts_with("main"), healthcare_utilization)


train_data <- ehr_data %>% filter(patient_id %in% data$training_set)
train_x <- train_data %>% dplyr::select(starts_with("COD"), starts_with("NLP"), 
                                 starts_with("main"), healthcare_utilization)
train_y <- train_data %>% dplyr::select(label) %>% pull()

test_data <- ehr_data %>% filter(patient_id %in% data$validation_set) 
test_x <- test_data %>% dplyr::select(starts_with("COD"), starts_with("NLP"), 
                               starts_with("main"), healthcare_utilization)
test_y <- test_data %>% dplyr::select(label) %>% pull()
```

Transform the features. 

```{r}
train_x <- log(train_x + 1)

test_x <- log(test_x + 1)
```

## Supervised learning. 

1. Penalized logistic regression 

```{r}
model_lr <- glmnet(train_x, train_y, family = "binomial", 
                   alpha = 1, nlambda = 100L, lambda.min.ratio = 1e-3)
```

2. Random forest 

```{r}
model_rf <- rfsrc(y ~., data = data.frame(y = train_y, x = train_x))
```

What else?

- We can try different number of labels. 
- Try n = 50, 75, 90

```{r}
idx50 <- sample(c(1:length(train_y)), 50, replace = FALSE)
idx75 <- sample(c(1:length(train_y)), 75, replace = FALSE)
idx90 <- sample(c(1:length(train_y)), 90, replace = FALSE)

model_lr50 <- glmnet(train_x[idx50, ], train_y[idx50], family = "binomial", 
                   alpha = 1, nlambda = 100L, lambda.min.ratio = 1e-3)

model_lr75 <- glmnet(train_x[idx75, ], train_y[idx75], family = "binomial", 
                   alpha = 1, nlambda = 100L, lambda.min.ratio = 1e-3)

model_lr90 <- glmnet(train_x[idx90, ], train_y[idx90], family = "binomial", 
                   alpha = 1, nlambda = 100L, lambda.min.ratio = 1e-3)
```


## Semi-supervised learning.

1. PheCAP (fit supervised model using select features)

```{r}
other_feature <- c("healthcare_utilization", "main_ICD", "main_NLP")
```

```{r}
modelssl_lr <- glmnet(train_x[, c(other_feature, SAFE_feature)], train_y, family = "binomial", 
                   alpha = 1, nlambda = 100L, lambda.min.ratio = 1e-3)
#coef(modelssl_lr)

modelssl_rf <- rfsrc(y ~., 
                     data = data.frame(y = train_y, train_x[, c(other_feature, SAFE_feature)]))
```

2. Two-step semi-supervised method. 

(i) Regress the surrogate on the features with penalized least square to get the direction of beta.
(ii) Regress the outcome on the linear predictor to get the intercept and multiplier for the beta. 

Step (i):

```{r}
model2ssl_step1 <- glmnet(x, snlp, alpha = 1, nlambda = 100L, lambda.min.ratio = 1e-3)
newy_original <- predict(model2ssl_lr, s = min(model2ssl_lr$lambda), newx = x)
```

Step (ii):

```{r}
newy <- newy_original[data$training_set]
model2ssl_step2 <- glm(train_y ~ newy + snlp[data$training_set] + 
                         sicd[data$training_set] + 
                         health_count[data$training_set], family = "binomial") 
```


```{r}
beta_step2 <- as.double(coef(model2ssl_step2))

newx_step2 <- cbind(newy_original, snlp, sicd, health_count)

score_step2 <- plogis(beta_step2[1L] + 
                        drop(newx_step2 %*% beta_step2[-1L]))
```

## Weakly-supervised learning. 

1. PheNorm 

The function required all data columns need to be log-transformed and need column names.

Already transformed main NLP and ICD as surrogates. 

Need to do one more time to healthcare utilization.

```{r}
health_count <- log(ehr_data$healthcare_utilization + 1)
```


```{r}
data_fit <- data.frame("main_NLP" = snlp, "main_ICD" = sicd, 
                       "healthcare_utilization" = health_count)
```

```{r}
model_phenorm <- PheNorm.Prob(
  nm.logS.ori = "main_ICD", 
  nm.utl = "healthcare_utilization", 
  dat = data_fit, 
  corrupt.rate = 0.3, 
  train.size = nrow(data_fit)
)
```

```{r}
head(model_phenorm$probs)
```

2. MAP 

```{r}
data_fit <- Matrix(cbind(ICD = ehr_data$main_ICD, NLP = ehr_data$main_NLP), sparse = TRUE)
note <- Matrix(ehr_data$healthcare_utilization, ncol = 1, sparse = TRUE)
model_map <- MAP(mat = data_fit, note = note, full.output = TRUE)
```

```{r}
head(model_map$scores)
```

------------------------------------------------------------------------------------------

# Module validation. 

Predict using test data and report the AUC. 

```{r}
model_predict <- function(model, x, penalized_lambda = NA) {
  
  if (!is.na(penalized_lambda)) {
     x <- as.matrix(x)
     beta_hat <- as.double(predict(model, x, s = penalized_lambda, type = "coefficients"))
  } else {
     #beta_hat <- as.double(predict(model, data.frame(x), type = "coefficients"))
     beta_hat <- model$coefficients
  }
  
 return(plogis(beta[1L] + drop(x %*% beta_hat[-1L])))
}
```

```{r}
#y_hat <- model_predict(model_lr, x = test_x, penalized_lambda = min(model_lr$lambda))
y_hat <- model_predict(modelssl_lr, 
                       x = test_x[, c(other_feature, SAFE_feature)], 
                       penalized_lambda = min(modelssl_lr$lambda))
```

```{r}
model_lr_roc <- get_roc(test_y, y_hat)
```

```{r}
head(model_lr_roc, 5)
```

```{r}
get_auc(test_y, y_hat)
```

```{r}
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)
```

# Supervised

```{r}
y_hat <- model_predict(model_lr50, 
                       x = test_x, 
                       penalized_lambda = min(model_lr50$lambda))
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)

y_hat <- model_predict(model_lr75, 
                       x = test_x, 
                       penalized_lambda = min(model_lr50$lambda))
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)

y_hat <- model_predict(model_lr90, 
                       x = test_x, 
                       penalized_lambda = min(model_lr50$lambda))
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)
```

# Semi-supervised

two-step

```{r}
y_hat <- score_step2[data$validation_set]
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)
```


```{r}
y_hat <- model_predict(modelssl_lr, 
                       x = test_x[, c(other_feature, SAFE_feature)], 
                       penalized_lambda = min(modelssl_lr$lambda))
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)
```

# Weakly-supervised

```{r}
y_hat <- model_phenorm$probs[data$validation_set]
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)
```


```{r}
y_hat <- model_map$scores[data$validation_set]
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)
```


