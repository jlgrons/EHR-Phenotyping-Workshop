---
title: "Module 2: Supervised learning"
output: beamer_presentation
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Load helper functions.
source("../Rscripts/helper_function.R")
```

```{r, include=FALSE}
# If a package is installed, it will be loaded. If any
## are not, the missing package(s) will be installed
## from CRAN and then loaded.

## First specify the packages of interest
packages <- c(
  "dplyr", "PheCAP", "glmnet", "randomForestSRC", "PheNorm",
  "MAP", "pROC", "mltools", "data.table", "ggplot2", "parallel"
)

## Now load or install&load all
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)

# load environment from example 1
load("environment.RData")
```


```{r include=FALSE}
data <- PhecapData(PheCAP::ehr_data, "healthcare_utilization", "label", 75,
  patient_id = "patient_id", seed = 123
)

# Data with non-missing labels
labeled_data <- ehr_data %>% dplyr::filter(!is.na(label))


# All Features
all_x <- ehr_data %>% dplyr::select(
  starts_with("COD"), starts_with("NLP"),
  starts_with("main"), healthcare_utilization
)
health_count <- ehr_data$healthcare_utilization

# Training Set
train_data <- ehr_data %>% dplyr::filter(patient_id %in% data$training_set)
train_x <- train_data %>%
  dplyr::select(
    starts_with("COD"), starts_with("NLP"),
    starts_with("main"), healthcare_utilization
  ) %>%
  as.matrix()
train_y <- train_data %>%
  dplyr::select(label) %>%
  pull()

# Testing Set
test_data <- ehr_data %>% dplyr::filter(patient_id %in% data$validation_set)
test_x <- test_data %>%
  dplyr::select(
    starts_with("COD"), starts_with("NLP"),
    starts_with("main"), healthcare_utilization
  ) %>%
  as.matrix()
test_y <- test_data %>%
  dplyr::select(label) %>%
  pull()
```


# Split data into train and test 

```{r}
dim(train_x)
```

```{r}
length(train_y)
```


```{r}
dim(test_x)
```

```{r}
length(test_y)
```

# LASSO logistic regression

```{r}
# Choose best lambda using CV
beta.lasso <- lasso_fit(
  x = train_x, y = train_y,
  tuning = "cv", family = "binomial"
)
```

\tiny
```{r}
# Features Selected
names(beta.lasso[abs(beta.lasso) > 0])[-1]
```

# ALASSO logistic regression

```{r}
# Fit Adaptive LASSO
beta.alasso <- adaptive_lasso_fit(
  x = train_x, y = train_y,
  tuning = "cv", family = "binomial"
)
```

\tiny
```{r}
# ALASSO features selected
names(beta.alasso[abs(beta.alasso) > 0])[-1]
```

```{r}
# LASSO features selected
names(beta.lasso[abs(beta.lasso) > 0])[-1]
```

# Get model predictions + ROC curve

```{r}
# Prediction on testing set (LASSO)
y_hat.lasso <- linear_model_predict(
  beta = beta.lasso, x = test_x,
  probability = TRUE
)
```

```{r}
# Prediction on testing set (ALASSO)
y_hat.alasso <- linear_model_predict(
  beta = beta.alasso, x = test_x,
  probability = TRUE
)
```

```{r}
roc.lasso <- roc(test_y, y_hat.lasso)
roc.alasso <- roc(test_y, y_hat.alasso)
# as expected alasso selects less features
```


# LASSO vs. ALASSO

```{r, echo = FALSE}
plot(roc.lasso,
  print.auc = TRUE, main = paste0("n_training = ", nrow(train_x))
)
plot(roc.alasso,
  print.auc = TRUE, col = "red", add = TRUE, print.auc.y = 0.4
)
legend(0, 0.2,
  legend = c("LASSO", "ALASSO"), col = c("black", "red"),
  lty = 1, cex = 0.8
)

## Please change ROC plots to TPR and FPR so it matches the output
## Unify the format for all plots -titles, colors, axis, etc
## Make legends bigger
## Make axis and axis marks bigger - titles - everything so it easy to see
## Use ggplot for all plots - use default colors
## Jianhui uses . for separating words in variables, Siyue uses _ : pick one, I prefer _
```

# LASSO vs. ALASSO at FPR = 0.10

\tiny
```{r}
roc_full.lasso <- get_roc(y_true = test_y, y_score = y_hat.lasso) %>% data.frame()
get_roc_parameter(0.1, roc_full.lasso)
```

```{r}
roc_full.alasso <- get_roc(y_true = test_y, y_score = y_hat.alasso) %>% data.frame()
get_roc_parameter(0.1, roc_full.alasso)
```
# LASSO vs. ALASSO with different training set size
- Randomly sample training size = 50, 70, 90
- Use the remaining data as the test set 
- Repeat 600 times

```{r, cache=TRUE}
auc_supervised <- validate_supervised(
  dat = labeled_data, nsim = 600,
  n.train = c(50, 70, 90)
)
```

# LASSO vs. ALASSO with different training set size

```{r, echo = FALSE}
par(mfrow = c(1, 3))
boxplot(auc_supervised %>% select(starts_with("n=50")),
  ylim = c(0.5, 1),
  names = c("LASSO", "ALASSO"), main = "n=50"
)
boxplot(auc_supervised %>% select(starts_with("n=70")),
  ylim = c(0.5, 1),
  names = c("LASSO", "ALASSO"), main = "n=70"
)
boxplot(auc_supervised %>% select(starts_with("n=90")),
  ylim = c(0.5, 1),
  names = c("LASSO", "ALASSO"), main = "n=90"
)

# Use ggplot
# Remove outlier for box plot
```

# Random Forest and SVM

\tiny
```{r}
# Random forest
model_rf <- rfsrc(y ~ ., data = data.frame(y = train_y, x = train_x))
y_hat.rf <- predict(model_rf, 
                    newdata = data.frame(x = test_x))$predicted
roc.rf <- roc(test_y, y_hat.rf)
# Use the Phecap functions to compute lasso, alssso etc and be consistnet
# clean the helper functions to only have what you need
# same for packages
# Make sure the slides match the markdown - please update
```

```{r}
# SVM
model_svm <- SVMMaj::svmmaj(X = train_x, y = train_y)
y_hat.svm <- predict(model_svm, test_x)
roc.svm <- roc(test_y, y_hat.svm)
```

# ROC curves

```{r, echo = FALSE}
roc.svm <- roc(test_y, y_hat.svm)

plot(roc.lasso,
  print.auc = TRUE, main = paste0("n_training = ", nrow(train_x))
)
plot(roc.alasso,
  print.auc = TRUE, col = "red", add = TRUE, print.auc.y = 0.4
)
plot(roc.rf,
  print.auc = TRUE, col = "grey", add = TRUE, print.auc.y = 0.3
)
plot(roc.svm,
  print.auc = TRUE, col = "pink", add = TRUE, print.auc.y = 0.2
)
legend(0, 0.5,
  legend = c("LASSO", "ALASSO", "Random Forest", "SVM"),
  col = c("black", "red", "grey", "pink"),
  lty = 1, cex = 0.5
)
```
