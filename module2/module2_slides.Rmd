---
title: "Module 2: Supervised learning"
output: beamer_presentation
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Load helper functions.
source("../Rscripts/helper_function.R")
```

```{r, include=FALSE}
# If a package is installed, it will be loaded. If any
## are not, the missing package(s) will be installed
## from CRAN and then loaded.

## First specify the packages of interest
packages <- c(
  "dplyr", "PheCAP", "glmnet", "randomForestSRC", "PheNorm",
  "MAP", "pROC", "mltools", "data.table", "ggplot2", "parallel"
)

## Now load or install&load all
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)
```


```{r include=FALSE}
data(ehr_data)
data <- PhecapData(PheCAP::ehr_data, "healthcare_utilization", "label", 75,
  patient_id = "patient_id", seed = 123
)

# Data with non-missing labels
labeled_data <- ehr_data %>% dplyr::filter(!is.na(label))


# All Features
all_x <- ehr_data %>% dplyr::select(
  starts_with("COD"), starts_with("NLP"),
  starts_with("main"), healthcare_utilization
)
health_count <- ehr_data$healthcare_utilization

# Training Set
train_data <- ehr_data %>% dplyr::filter(patient_id %in% data$training_set)
train_x <- train_data %>%
  dplyr::select(
    starts_with("COD"), starts_with("NLP"),
    starts_with("main"), healthcare_utilization
  ) %>%
  as.matrix()
train_y <- train_data %>%
  dplyr::select(label) %>%
  pull()

# Testing Set
test_data <- ehr_data %>% dplyr::filter(patient_id %in% data$validation_set)
test_x <- test_data %>%
  dplyr::select(
    starts_with("COD"), starts_with("NLP"),
    starts_with("main"), healthcare_utilization
  ) %>%
  as.matrix()
test_y <- test_data %>%
  dplyr::select(label) %>%
  pull()
```


# Split data into train and test 

```{r}
dim(train_x)
```

```{r}
length(train_y)
```


```{r}
dim(test_x)
```

```{r}
length(test_y)
```

# LASSO logistic regression

```{r}
# Choose best lambda using CV
beta_lasso <- lasso_fit(
  x = log(train_x + 1), y = train_y,
  tuning = "cv", family = "binomial"
)
```

\tiny
```{r}
# Features Selected
names(beta_lasso[abs(beta_lasso) > 0])[-1]
```

# ALASSO logistic regression
\tiny
```{r}
# Fit Adaptive LASSO
beta_alasso <- adaptive_lasso_fit(
  x = log(train_x + 1), y = train_y,
  tuning = "cv", family = "binomial"
)
```


```{r}
# ALASSO features selected 
beta_alasso[!beta_alasso == 0][-1]
```

```{r}
# LASSO features selected
beta_lasso[!beta_lasso==0][-1]
```

# Get model predictions + ROC curve

```{r}
# Prediction on testing set (LASSO)
y_hat_lasso <- linear_model_predict(
  beta = beta_lasso, x = log(test_x + 1),
  probability = TRUE
)
```

```{r}
# Prediction on testing set (ALASSO)
y_hat_alasso <- linear_model_predict(
  beta = beta_alasso, x = log(test_x + 1),
  probability = TRUE
)
```

```{r}
roc_lasso <- roc(test_y, y_hat_lasso)
roc_alasso <- roc(test_y, y_hat_alasso)
# as expected alasso selects less features
```


# LASSO vs. ALASSO

```{r, echo = FALSE}
ggroc(list(roc_lasso, roc_alasso), legacy.axes = TRUE) +
  scale_color_discrete(labels = c(
    paste("LASSO, AUC =", round(auc(roc_lasso), 3)),
    paste("ALASSO, AUC =", round(auc(roc_alasso), 3))
  )) + theme(legend.position = "bottom") +
  ggtitle("The operating receiver characteristic (ROC) curve")

## Please change ROC plots to TPR and FPR so it matches the output
## Unify the format for all plots -titles, colors, axis, etc
## Make legends bigger
## Make axis and axis marks bigger - titles - everything so it easy to see
## Use ggplot for all plots - use default colors
## Jianhui uses . for separating words in variables, Siyue uses _ : pick one, I prefer _
```

# LASSO vs. ALASSO at FPR = 0.10

\tiny
```{r}
roc_full_lasso <- get_roc(y_true = test_y, y_score = y_hat_lasso) %>% data.frame()
get_roc_parameter(0.1, roc_full_lasso)
```

```{r}
roc_full_alasso <- get_roc(y_true = test_y, y_score = y_hat_alasso) %>% data.frame()
get_roc_parameter(0.1, roc_full_alasso)
```
# LASSO vs. ALASSO with different training set size
- Randomly sample training size = 50, 70, 90
- Use the remaining data as the test set 
- Repeat 600 times

```{r, cache=TRUE}
auc_supervised <- validate_supervised(
  dat = labeled_data, nsim = 600,
  n.train = c(50, 70, 90)
)
```

# LASSO vs. ALASSO with different training set size

```{r, echo = FALSE}
tidyr::gather(auc_supervised) %>%
  mutate(
    n = gsub(",.*$", "", key),
    method = sub(".*,\\s*", "", key)
  ) %>%
  ggplot(aes(y = value, color = method)) +
  geom_boxplot(outlier.shape = NA) +
  facet_wrap(. ~ n) +
  ggtitle("Area under the ROC curve (AUC) from 600 simulations") +
  theme(legend.position = "bottom", axis.text.x = element_blank(), axis.ticks = element_blank())

# Use ggplot
# Remove outlier for box plot
```

# Random Forest and SVM

\tiny
```{r}
# Random forest
model_rf <- rfsrc(y ~ ., data = data.frame(y = train_y, x = train_x))
y_hat_rf <- predict(model_rf,
  newdata = data.frame(x = test_x)
)$predicted
roc_rf <- roc(test_y, y_hat_rf)
```

```{r}
# SVM
model_svm <- SVMMaj::svmmaj(X = train_x, y = train_y)
y_hat_svm <- predict(model_svm, test_x)
roc_svm <- roc(test_y, y_hat_svm)
```

# ROC curves

```{r, echo = FALSE}
ggroc(list(roc_lasso, roc_alasso, roc_rf, roc_svm), legacy.axes = TRUE) +
  scale_color_discrete(labels = c(
    paste("LASSO, AUC =", round(auc(roc_lasso), 3)),
    paste("Random Forest, AUC =", round(auc(roc_rf), 3)),
    paste("ALASSO, AUC =", round(auc(roc_alasso), 3)),
    paste("SVM, AUC =", round(auc(roc_svm), 3))
  )) + theme(legend.position = "bottom") +
  ggtitle("The operating receiver characteristic (ROC) curve") + 
  guides(color = guide_legend(ncol = 2))
```
