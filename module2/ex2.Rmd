---
title: "ex2"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(randomForestSRC)
library(PheNorm)
library(MAP)
```

# Module 2: Feature selection. 

## Prepare data for algorithm development 

- Split data into training and testing set

```{r}
all_x <- ehr_data %>% dplyr::select(starts_with("COD"), starts_with("NLP"), 
                                 starts_with("main"), healthcare_utilization)


train_data <- ehr_data %>% filter(patient_id %in% data$training_set)
train_x <- train_data %>% dplyr::select(starts_with("COD"), starts_with("NLP"), 
                                 starts_with("main"), healthcare_utilization)
train_y <- train_data %>% dplyr::select(label) %>% pull()

test_data <- ehr_data %>% filter(patient_id %in% data$validation_set) 
test_x <- test_data %>% dplyr::select(starts_with("COD"), starts_with("NLP"), 
                               starts_with("main"), healthcare_utilization)
test_y <- test_data %>% dplyr::select(label) %>% pull()
```

Transform the features. 

```{r}
train_x <- log(train_x + 1)

test_x <- log(test_x + 1)
```

## Supervised learning. 

1. Penalized logistic regression 

```{r}
model_lr <- glmnet(train_x, train_y, family = "binomial", 
                   alpha = 1, nlambda = 100L, lambda.min.ratio = 1e-3)
```

2. Random forest 

```{r}
model_rf <- rfsrc(y ~., data = data.frame(y = train_y, x = train_x))
```

What else?

- We can try different number of labels. 
- Try n = 50, 75, 90

```{r}
idx50 <- sample(c(1:length(train_y)), 50, replace = FALSE)
idx75 <- sample(c(1:length(train_y)), 75, replace = FALSE)
idx90 <- sample(c(1:length(train_y)), 90, replace = FALSE)

model_lr50 <- glmnet(train_x[idx50, ], train_y[idx50], family = "binomial", 
                   alpha = 1, nlambda = 100L, lambda.min.ratio = 1e-3)

model_lr75 <- glmnet(train_x[idx75, ], train_y[idx75], family = "binomial", 
                   alpha = 1, nlambda = 100L, lambda.min.ratio = 1e-3)

model_lr90 <- glmnet(train_x[idx90, ], train_y[idx90], family = "binomial", 
                   alpha = 1, nlambda = 100L, lambda.min.ratio = 1e-3)
```


## Semi-supervised learning.

1. PheCAP (fit supervised model using select features)

```{r}
other_feature <- c("healthcare_utilization", "main_ICD", "main_NLP")
```

```{r}
modelssl_lr <- glmnet(train_x[, c(other_feature, SAFE_feature)], train_y, family = "binomial", 
                   alpha = 1, nlambda = 100L, lambda.min.ratio = 1e-3)
#coef(modelssl_lr)

modelssl_rf <- rfsrc(y ~., 
                     data = data.frame(y = train_y, train_x[, c(other_feature, SAFE_feature)]))
```

2. Two-step semi-supervised method. 

(i) Regress the surrogate on the features with penalized least square to get the direction of beta.
(ii) Regress the outcome on the linear predictor to get the intercept and multiplier for the beta. 

Step (i):

```{r}
model2ssl_step1 <- glmnet(x, snlp, alpha = 1, nlambda = 100L, lambda.min.ratio = 1e-3)
newy_original <- predict(model2ssl_lr, s = min(model2ssl_lr$lambda), newx = x)
```

Step (ii):

```{r}
newy <- newy_original[data$training_set]
model2ssl_step2 <- glm(train_y ~ newy + snlp[data$training_set] + 
                         sicd[data$training_set] + 
                         health_count[data$training_set], family = "binomial") 
```


```{r}
beta_step2 <- as.double(coef(model2ssl_step2))

newx_step2 <- cbind(newy_original, snlp, sicd, health_count)

score_step2 <- plogis(beta_step2[1L] + 
                        drop(newx_step2 %*% beta_step2[-1L]))
```

## Weakly-supervised learning. 

1. PheNorm 

The function required all data columns need to be log-transformed and need column names.

Already transformed main NLP and ICD as surrogates. 

Need to do one more time to healthcare utilization.

```{r}
health_count <- log(ehr_data$healthcare_utilization + 1)
```


```{r}
data_fit <- data.frame("main_NLP" = snlp, "main_ICD" = sicd, 
                       "healthcare_utilization" = health_count)
```

```{r}
model_phenorm <- PheNorm.Prob(
  nm.logS.ori = "main_ICD", 
  nm.utl = "healthcare_utilization", 
  dat = data_fit, 
  corrupt.rate = 0.3, 
  train.size = nrow(data_fit)
)
```

```{r}
head(model_phenorm$probs)
```

2. MAP 

```{r}
data_fit <- Matrix(cbind(ICD = ehr_data$main_ICD, NLP = ehr_data$main_NLP), sparse = TRUE)
note <- Matrix(ehr_data$healthcare_utilization, ncol = 1, sparse = TRUE)
model_map <- MAP(mat = data_fit, note = note, full.output = TRUE)
```

```{r}
head(model_map$scores)
```

------------------------------------------------------------------------------------------



# Module validation. 

Predict using test data and report the AUC. 

```{r}
model_predict <- function(model, x, penalized_lambda = NA) {
  
  if (!is.na(penalized_lambda)) {
     x <- as.matrix(x)
     beta_hat <- as.double(predict(model, x, s = penalized_lambda, type = "coefficients"))
  } else {
     #beta_hat <- as.double(predict(model, data.frame(x), type = "coefficients"))
     beta_hat <- model$coefficients
  }
  
 return(plogis(beta[1L] + drop(x %*% beta_hat[-1L])))
}
```

```{r}
#y_hat <- model_predict(model_lr, x = test_x, penalized_lambda = min(model_lr$lambda))
y_hat <- model_predict(modelssl_lr, 
                       x = test_x[, c(other_feature, SAFE_feature)], 
                       penalized_lambda = min(modelssl_lr$lambda))
```

```{r}
model_lr_roc <- get_roc(test_y, y_hat)
```

```{r}
head(model_lr_roc, 5)
```

```{r}
get_auc(test_y, y_hat)
```

```{r}
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)
```

# Supervised

```{r}
y_hat <- model_predict(model_lr50, 
                       x = test_x, 
                       penalized_lambda = min(model_lr50$lambda))
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)

y_hat <- model_predict(model_lr75, 
                       x = test_x, 
                       penalized_lambda = min(model_lr50$lambda))
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)

y_hat <- model_predict(model_lr90, 
                       x = test_x, 
                       penalized_lambda = min(model_lr50$lambda))
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)
```

# Semi-supervised

two-step

```{r}
y_hat <- score_step2[data$validation_set]
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)
```


```{r}
y_hat <- model_predict(modelssl_lr, 
                       x = test_x[, c(other_feature, SAFE_feature)], 
                       penalized_lambda = min(modelssl_lr$lambda))
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)
```

# Weakly-supervised

```{r}
y_hat <- model_phenorm$probs[data$validation_set]
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)
```


```{r}
y_hat <- model_map$scores[data$validation_set]
plot(roc(test_y, y_hat), print.auc=TRUE, max.auc.polygon=TRUE)
```


