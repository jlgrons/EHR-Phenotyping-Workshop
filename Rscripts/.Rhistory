beta_step2 <- coef(step2)
beta_step2
# recover beta
beta <- beta_step2[2] * beta.step1
# mu
mu <- beta_step2[1] +
as.numeric(as.matrix(x[test_data$patient_id, ]) %*% beta[-1]) +
as.numeric(beta_step2[3] %*% S[test_data$patient_id])
# expit
y_hat.ss <- plogis(mu)
plot(roc(test_y, y_hat.lasso),
print.auc = TRUE, main = "n_training = 106 (60%)"
)
plot(roc(test_y, y_hat.alasso),
print.auc = TRUE, col = "red", add = TRUE, print.auc.y = 0.4
)
plot(roc(test_y, y_hat.ss),
print.auc = TRUE, col = "green", add = TRUE, print.auc.y = 0.2
)
plot(roc(test_y, y_hat.phecap),
print.auc = TRUE, col = "blue", add = TRUE, print.auc.y = 0.3
)
legend(0, 0.3,
legend = c("LASSO", "ALASSO", "PheCAP", "Two-Step"),
col = c("black", "red", "blue", "green"),
lty = 1, cex = 0.8
)
ss.roc.full <- get_roc(test_y, y_hat.ss) %>% data.frame()
get_roc_parameter(0.05, ss.roc.full)
get_roc_parameter(0.1, ss.roc.full)
start <- Sys.time()
auc_twostep <- validate_ss(
dat = labeled_data, nsim = 600,
n.train = c(50, 70, 90),
beta = beta.step1,
S = S,
x = x
)
end <- Sys.time()
end - start
# median AUC
apply(auc_twostep, 2, median)
# se
apply(auc_twostep, 2, sd)
boxplot(auc_twostep,
ylim = c(0.5, 1), names = c("n=50", "n=70", "n=90"),
main = "Two-Step Semi-Supervised"
)
# Compare with Previous method
boxplot(cbind(auc_supervised, auc_phecap, auc_twostep)
%>% select(starts_with("n=50")),
ylim = c(0.5, 1), names = c("LASSO", "ALASSO","PheCAP", "Two-Step")
, main = "n=50"
)
boxplot(cbind(auc_supervised, auc_phecap, auc_twostep)
%>% select(starts_with("n=70")),
ylim = c(0.5, 1), names = c("LASSO", "ALASSO","PheCAP", "Two-Step"),
main = "n=70"
)
boxplot(cbind(auc_supervised, auc_phecap, auc_twostep)
%>% select(starts_with("n=90")),
ylim = c(0.5, 1), names = c("LASSO", "ALASSO","PheCAP","Two-Step"),
main = "n=90"
)
model_phenorm <- PheNorm.Prob(
nm.logS.ori = "main_ICDNLP", # name of surrogates
nm.utl = "healthcare_utilization", # name of HU
nm.X = colnames(ehr_data)[-1:-6], # Other predictors X
dat = ehr_data %>% select(-patient_id, -main_ICD, -main_NLP),
train.size = nrow(ehr_data)
)
y_hat.phenorm <- model_phenorm$probs[data$validation_set]
plot(roc(test_y, y_hat.lasso),
print.auc = TRUE, main = "n_training = 106 (60%)"
)
plot(roc(test_y, y_hat.alasso),
print.auc = TRUE, col = "red", add = TRUE, print.auc.y = 0.4
)
plot(roc(test_y, y_hat.phecap),
print.auc = TRUE, col = "blue", add = TRUE, print.auc.y = 0.3
)
plot(roc(test_y, y_hat.ss),
print.auc = TRUE, col = "green", add = TRUE, print.auc.y = 0.2
)
plot(roc(test_y, y_hat.phenorm),
print.auc = TRUE, col = "orange", add = TRUE, print.auc.y = 0.1
)
legend(0, 0.4,
legend = c("LASSO", "ALASSO", "PheCAP","Two-Step", "PheNorm"),
col = c("black", "red", "blue", "green", "orange"),
lty = 1, cex = 0.8
)
# Use untransformed data; MAP requires sparse matrix
# Create sparse matrix for surroagtes
data_fit <- sparsify(PheCAP::ehr_data %>%
select(main_ICD, main_NLP) %>%
rename(ICD = main_ICD) %>% data.table())
# Create sparse matrix for HU
note <- Matrix(PheCAP::ehr_data$healthcare_utilization, ncol = 1, sparse = TRUE)
model_map <- MAP(mat = data_fit, note = note, full.output = TRUE)
y_hat.map <- model_map$scores[data$validation_set]
plot(roc(test_y, y_hat.lasso),
print.auc = TRUE, main = "n_training = 106 (60%)"
)
plot(roc(test_y, y_hat.alasso),
print.auc = TRUE, col = "red", add = TRUE, print.auc.y = 0.4
)
plot(roc(test_y, y_hat.phecap),
print.auc = TRUE, col = "blue", add = TRUE, print.auc.y = 0.3
)
plot(roc(test_y, y_hat.ss),
print.auc = TRUE, col = "green", add = TRUE, print.auc.y = 0.2
)
plot(roc(test_y, y_hat.phenorm),
print.auc = TRUE, col = "orange", add = TRUE, print.auc.y = 0.1
)
plot(roc(test_y, y_hat.map),
print.auc = TRUE, col = "orchid", add = TRUE, print.auc.y = 0
)
legend(0, 0.4,
legend = c("LASSO", "ALASSO", "PheCAP","Two-Step",  "PheNorm", "MAP"),
col = c("black", "red", "blue", "green", "orange", "orchid"),
lty = 1, cex = 0.8
)
ehr_data
data_fit <- sparsify(PheCAP::ehr_data %>%
select(main_ICD, main_NLP, main_ICDNLP) %>%
rename(ICD = main_ICD) %>% data.table())
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Load helper functions.
source("../Rscripts/helper_function.R")
# If a package is installed, it will be loaded. If any
## are not, the missing package(s) will be installed
## from CRAN and then loaded.
## First specify the packages of interest
packages <- c(
"dplyr", "PheCAP", "glmnet", "randomForestSRC", "PheNorm",
"MAP", "pROC", "mltools", "data.table", "ggplot2", "parallel"
)
## Now load or install&load all
package.check <- lapply(
packages,
FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
}
)
# load environment from example 1
load("environment.RData")
ehr_data <- PhecapData(PheCAP::ehr_data, "healthcare_utilization", "label", 75,
patient_id = "patient_id", seed = 123
)
# Data with non-missing labels
labeled_data <- ehr_data %>% dplyr::filter(!is.na(label))
data(ehr_data)
data <- PhecapData(PheCAP::ehr_data, "healthcare_utilization", "label", 75,
patient_id = "patient_id", seed = 123
)
# Data with non-missing labels
labeled_data <- ehr_data %>% dplyr::filter(!is.na(label))
# All Features
all_x <- ehr_data %>% dplyr::select(
starts_with("COD"), starts_with("NLP"),
starts_with("main"), healthcare_utilization
)
health_count <- ehr_data$healthcare_utilization
# Training Set
train_data <- ehr_data %>% dplyr::filter(patient_id %in% data$training_set)
train_x <- train_data %>%
dplyr::select(
starts_with("COD"), starts_with("NLP"),
starts_with("main"), healthcare_utilization
) %>%
as.matrix()
train_y <- train_data %>%
dplyr::select(label) %>%
pull()
# Testing Set
test_data <- ehr_data %>% dplyr::filter(patient_id %in% data$validation_set)
test_x <- test_data %>%
dplyr::select(
starts_with("COD"), starts_with("NLP"),
starts_with("main"), healthcare_utilization
) %>%
as.matrix()
test_y <- test_data %>%
dplyr::select(label) %>%
pull()
headhead(train_x[, 1:5])
head(train_x[, 1:5])
# Choose best lambda using CV
beta.lasso <- lasso_fit(
x = log(train_x + 1), y = train_y,
tuning = "cv", family = "binomial"
)
beta.lasso
# Features Selected
names(beta.lasso[abs(beta.lasso) > 0])[-1]
# Fit Adaptive LASSO
beta.alasso <- adaptive_lasso_fit(
x = log(train_x + 1), y = train_y,
tuning = "cv", family = "binomial"
)
# ALASSO features selected
names(beta.alasso[abs(beta.alasso) > 0])[-1]
# LASSO features selected
names(beta.lasso[abs(beta.lasso) > 0])[-1]
# Prediction on testing set (LASSO)
y_hat.lasso <- linear_model_predict(
beta = beta.lasso, x = log(test_x + 1),
probability = TRUE
)
# Prediction on testing set (ALASSO)
y_hat.alasso <- linear_model_predict(
beta = beta.alasso, x = log(test_x + 1),
probability = TRUE
)
roc.lasso <- roc(test_y, y_hat.lasso)
roc.alasso <- roc(test_y, y_hat.alasso)
# as expected alasso selects less features
plot(roc.lasso,
print.auc = TRUE, main = paste0("n_training = ", nrow(train_x))
)
plot(roc.alasso,
print.auc = TRUE, col = "red", add = TRUE, print.auc.y = 0.4
)
legend(0, 0.2,
legend = c("LASSO", "ALASSO"), col = c("black", "red"),
lty = 1, cex = 0.8
)
## Please change ROC plots to TPR and FPR so it matches the output
## Unify the format for all plots -titles, colors, axis, etc
## Make legends bigger
## Make axis and axis marks bigger - titles - everything so it easy to see
## Use ggplot for all plots - use default colors
## Jianhui uses . for separating words in variables, Siyue uses _ : pick one, I prefer _
colnames(x)
colnames(train_x)
# If a package is installed, it will be loaded. If any
## are not, the missing package(s) will be installed
## from CRAN and then loaded.
## First specify the packages of interest
packages <- c(
"dplyr", "PheCAP", "glmnet", "randomForestSRC", "PheNorm",
"MAP", "pROC", "mltools", "data.table", "ggplot2", "parallel"
)
## Now load or install&load all
package.check <- lapply(
packages,
FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
}
)
# load environment from example 1
load("../module4/environment.RData")
source("../Rscripts/helper_function.R")
# format slides nicely please
data(ehr_data)
data <- PhecapData(PheCAP::ehr_data, "healthcare_utilization", "label", 75,
patient_id = "patient_id", seed = 123
)
# Data with non-missing labels
labeled_data <- ehr_data %>% dplyr::filter(!is.na(label))
# All Features
all_x <- ehr_data %>% dplyr::select(
starts_with("COD"), starts_with("NLP"),
starts_with("main"), healthcare_utilization
)
health_count <- ehr_data$healthcare_utilization
# Training Set
train_data <- ehr_data %>% dplyr::filter(patient_id %in% data$training_set)
train_x <- train_data %>%
dplyr::select(
starts_with("COD"), starts_with("NLP"),
starts_with("main"), healthcare_utilization
) %>%
as.matrix()
train_y <- train_data %>%
dplyr::select(label) %>%
pull()
# Testing Set
test_data <- ehr_data %>% dplyr::filter(patient_id %in% data$validation_set)
test_x <- test_data %>%
dplyr::select(
starts_with("COD"), starts_with("NLP"),
starts_with("main"), healthcare_utilization
) %>%
as.matrix()
test_y <- test_data %>%
dplyr::select(label) %>%
pull()
x <- all_x %>% select(starts_with("health") | starts_with("COD") | starts_with("NLP")) # COD + NLP
S <- ehr_data$main_ICDNLP
# Step 1
beta.step1 <- adaptive_lasso_fit(
y = S, # surrogate
x = x, # all X
family = "gaussian",
tuning = "cv"
)
S
x
x <- all_x %>% select(starts_with("health") | starts_with("COD") | starts_with("NLP")) # COD + NLP
S <- ehr_data$main_ICD + ehr_data$main_NLP
# Step 1
beta.step1 <- adaptive_lasso_fit(
y = S, # surrogate
x = x, # all X
family = "gaussian",
tuning = "cv"
)
# Features selected
names(beta.step1[abs(beta.step1) > 0])[-1]
# linear predictor without intercept
bhatx <- linear_model_predict(beta = beta.step1, x = as.matrix(x))
# Step 2
step2 <- glm(train_y ~ bhatx[train_data$patient_id] + S[train_data$patient_id] +
health_count[train_data$patient_id])
beta_step2 <- coef(step2)
beta_step2
# recover beta
beta <- beta_step2[2] * beta.step1
# mu
mu <- beta_step2[1] +
as.numeric(as.matrix(x[test_data$patient_id, ]) %*% beta[-1]) +
as.numeric(beta_step2[3] %*% S[test_data$patient_id]) +
as.numeric(beta_step2[4] %*% health_count[test_data$patient_id])
# expit
y_hat.ss <- plogis(mu)
# add feature comparison here to lasso, alsso, phecap
# recover beta
beta <- beta_step2[2] * beta.step1
# mu
mu <- beta_step2[1] +
as.numeric(as.matrix(x[test_data$patient_id, ]) %*% beta[-1]) +
as.numeric(beta_step2[3] %*% S[test_data$patient_id]) +
as.numeric(beta_step2[4] %*% health_count[test_data$patient_id])
# expit
y_hat.ss <- plogis(mu)
# add feature comparison here to lasso, alsso, phecap
plot(roc(test_y, y_hat.lasso),
print.auc = TRUE, main = "n_training = 106 (60%)"
)
plot(roc(test_y, y_hat.alasso),
print.auc = TRUE, col = "red", add = TRUE, print.auc.y = 0.4
)
plot(roc(test_y, y_hat.ss),
print.auc = TRUE, col = "green", add = TRUE, print.auc.y = 0.2
)
plot(roc(test_y, y_hat.phecap),
print.auc = TRUE, col = "blue", add = TRUE, print.auc.y = 0.3
)
legend(0, 0.3,
legend = c("LASSO", "ALASSO", "PheCAP", "Two-Step"),
col = c("black", "red", "blue", "green"),
lty = 1, cex = 0.8
)
x <- all_x %>% select(starts_with("health") | starts_with("COD") | starts_with("NLP")) # COD + NLP
S <- ehr_data$main_ICD + ehr_data$main_NLP
# Step 1
beta.step1 <- adaptive_lasso_fit(
y = S, # surrogate
x = x, # all X
family = "gaussian",
tuning = "bic"
)
x <- all_x %>% select(starts_with("health") | starts_with("COD") | starts_with("NLP")) # COD + NLP
S <- ehr_data$main_ICD + ehr_data$main_NLP
# Step 1
beta.step1 <- adaptive_lasso_fit(
y = S, # surrogate
x = x, # all X
family = "gaussian",
tuning = "ic"
)
# Features selected
names(beta.step1[abs(beta.step1) > 0])[-1]
# linear predictor without intercept
bhatx <- linear_model_predict(beta = beta.step1, x = as.matrix(x))
# Step 2
step2 <- glm(train_y ~ bhatx[train_data$patient_id] + S[train_data$patient_id] +
health_count[train_data$patient_id])
beta_step2 <- coef(step2)
beta_step2
x <- all_x %>% select(starts_with("health") | starts_with("COD") | starts_with("NLP")) # COD + NLP
S <- ehr_data$main_ICD + ehr_data$main_NLP
# Step 1
beta.step1 <- adaptive_lasso_fit(
y = S, # surrogate
x = x, # all X
family = "gaussian",
tuning = "cv"
)
# Features selected
names(beta.step1[abs(beta.step1) > 0])[-1]
# linear predictor without intercept
bhatx <- linear_model_predict(beta = beta.step1, x = as.matrix(x))
# Step 2
step2 <- glm(train_y ~ bhatx[train_data$patient_id] + S[train_data$patient_id] +
health_count[train_data$patient_id], family = "binomial")
beta_step2 <- coef(step2)
beta_step2
# recover beta
beta <- beta_step2[2] * beta.step1
# mu
mu <- beta_step2[1] +
as.numeric(as.matrix(x[test_data$patient_id, ]) %*% beta[-1]) +
as.numeric(beta_step2[3] %*% S[test_data$patient_id]) +
as.numeric(beta_step2[4] %*% health_count[test_data$patient_id])
# expit
y_hat.ss <- plogis(mu)
# add feature comparison here to lasso, alsso, phecap
plot(roc(test_y, y_hat.lasso),
print.auc = TRUE, main = "n_training = 106 (60%)"
)
plot(roc(test_y, y_hat.alasso),
print.auc = TRUE, col = "red", add = TRUE, print.auc.y = 0.4
)
plot(roc(test_y, y_hat.ss),
print.auc = TRUE, col = "green", add = TRUE, print.auc.y = 0.2
)
plot(roc(test_y, y_hat.phecap),
print.auc = TRUE, col = "blue", add = TRUE, print.auc.y = 0.3
)
legend(0, 0.3,
legend = c("LASSO", "ALASSO", "PheCAP", "Two-Step"),
col = c("black", "red", "blue", "green"),
lty = 1, cex = 0.8
)
x <- all_x %>% select(starts_with("COD") | starts_with("NLP")) # COD + NLP
S <- ehr_data$main_ICD + ehr_data$main_NLP
# Step 1
beta.step1 <- adaptive_lasso_fit(
y = S, # surrogate
x = x, # all X
family = "gaussian",
tuning = "cv"
)
# Features selected
names(beta.step1[abs(beta.step1) > 0])[-1]
x <- all_x %>% select(starts_with("health") | starts_with("COD") | starts_with("NLP")) # COD + NLP
S <- ehr_data$main_ICD + ehr_data$main_NLP
# Step 1
beta.step1 <- adaptive_lasso_fit(
y = S[1:5000], # surrogate
x = x[1:5000,], # all X
family = "gaussian",
tuning = "bic"
)
x <- all_x %>% select(starts_with("health") | starts_with("COD") | starts_with("NLP")) # COD + NLP
S <- ehr_data$main_ICD + ehr_data$main_NLP
# Step 1
beta.step1 <- adaptive_lasso_fit(
y = S[1:5000], # surrogate
x = x[1:5000,], # all X
family = "gaussian",
tuning = "cv"
)
# Features selected
names(beta.step1[abs(beta.step1) > 0])[-1]
S
x
x <- log(all_x %>% select(starts_with("health") | starts_with("COD") | starts_with("NLP")) + 1) # COD + NLP
S <- log(ehr_data$main_ICD + ehr_data$main_NLP + 1)
# Step 1
beta.step1 <- adaptive_lasso_fit(
y = S[], # surrogate
x = x[], # all X
family = "gaussian",
tuning = "cv"
)
# Features selected
names(beta.step1[abs(beta.step1) > 0])[-1]
# linear predictor without intercept
bhatx <- linear_model_predict(beta = beta.step1, x = as.matrix(x))
# Step 2
step2 <- glm(train_y ~ bhatx[train_data$patient_id] + S[train_data$patient_id] +
health_count[train_data$patient_id])
beta_step2 <- coef(step2)
beta_step2
# recover beta
beta <- beta_step2[2] * beta.step1
# mu
mu <- beta_step2[1] +
as.numeric(as.matrix(x[test_data$patient_id, ]) %*% beta[-1]) +
as.numeric(beta_step2[3] %*% S[test_data$patient_id]) +
as.numeric(beta_step2[4] %*% health_count[test_data$patient_id])
# expit
y_hat.ss <- plogis(mu)
# add feature comparison here to lasso, alsso, phecap
plot(roc(test_y, y_hat.lasso),
print.auc = TRUE, main = "n_training = 106 (60%)"
)
plot(roc(test_y, y_hat.alasso),
print.auc = TRUE, col = "red", add = TRUE, print.auc.y = 0.4
)
plot(roc(test_y, y_hat.ss),
print.auc = TRUE, col = "green", add = TRUE, print.auc.y = 0.2
)
plot(roc(test_y, y_hat.phecap),
print.auc = TRUE, col = "blue", add = TRUE, print.auc.y = 0.3
)
legend(0, 0.3,
legend = c("LASSO", "ALASSO", "PheCAP", "Two-Step"),
col = c("black", "red", "blue", "green"),
lty = 1, cex = 0.8
)
